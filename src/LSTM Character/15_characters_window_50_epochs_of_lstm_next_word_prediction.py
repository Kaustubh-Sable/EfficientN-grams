# -*- coding: utf-8 -*-
"""15 characters window 50 epochs of LSTM Next word Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mvOcZ__Z6GpBLww-l-kuyETzN4lqb6KA

**LSTM Based N-gram predictor**    

In this model - given past n characters, we try to predict the next 'k' characters. Thus, by parsing the output to the first ' ' or space character, we
can obtain the next predicted workd.

Pros - 

1.   This model can be helpful for also predicting the remaining characters of 
  a word given first few characters of it.
2.   This model is computationally less intensive, as the output layer has only a few output choices unlike the word based model which has a huge output softmax (which is computationally expensive). Here the output softmax has only the size equal to that of the number of unique characters in the given language which is very low as compared to all pollible dictionary words. 

Cons - 


1.   The output of this model needs post processing, as the model outputs a sequence of characters, one needs to parse the word and check if it is valid by a dictionary lookup.
"""

import sys
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils

from google.colab import drive
drive.mount('/gdrive')

import os

print(os.listdir('../gdrive/My Drive/NLP'))
os.chdir('../gdrive/My Drive/NLP/')

!pip install tensorflow-gpu==1.14

# load ascii text and covert to lowercase
filename = "LSTM/republic.txt"
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()

# create mapping of unique chars to integers, and a reverse mapping
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

# summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)
print ("Total Characters: ", n_chars)
print ("Total Vocab: ", n_vocab)

# prepare the dataset of input to output pairs encoded as integers
seq_length = 10
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
	seq_in = raw_text[i:i + seq_length]
	seq_out = raw_text[i + seq_length]
	dataX.append([char_to_int[char] for char in seq_in])
	dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)

n_patterns = len(dataX)
print ("Total Patterns: ", n_patterns)

# reshape X to be [samples, time steps, features]
X = numpy.reshape(dataX, (n_patterns, seq_length, 1))
# normalize
X = X / float(n_vocab)

# one hot encode the output variable
y = np_utils.to_categorical(dataY)
# define the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')
# define the checkpoint
filepath="weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
history = model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list)

# serialize model to JSON
from keras.models import Sequential
from keras.layers import Dense
from keras.models import model_from_json
import numpy
import os

model_json = model.to_json()
with open("model_10_char.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model_10_char.h5")
print("Saved model to disk")

from keras.utils import plot_model
plot_model(model, to_file='model.png')

import matplotlib.pyplot as plt

#history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)

# Plot training & validation accuracy values
"""plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()"""
"""
# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()"""

model.summary()

# load the network weights
filename = "model_50.h5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')

model

# We will remove double quotes, !, ? $, #, etc
# specify to translate chars 
str1 = ""
# specify to replace with 
str2 = ""
# delete chars 
str3 = "\"!?#$%^&*+"

trg = raw_text
table = trg.maketrans(str1, str2, str3)
raw_text = trg.translate(table)

print(len(raw_text))


words = raw_text.split()
print (len(words))

words = [word.lower() for word in words]

import nltk
freq_dist = nltk.FreqDist(words)


from nltk.util import ngrams

esBigrams = ngrams(words, 2)


import collections
esBigramFreq = collections.Counter(esBigrams)


esTrigrams = ngrams(words, 3)
esTrigramFreq = collections.Counter(esTrigrams)

esTrigramFreq.most_common(10)

esBigramFreq.most_common(1000)

esTrigramFreq.most_common(10)

prev = [char_to_int[value] for value in "of "]
temp = [1]*10
temp[-len(prev):] = prev
patternX = temp

"""# Test Suite"""

tests = esBigramFreq.most_common(100)

antecedents = []
consequents = []
for k in tests:
  antecedents.append(k[0][0])
  consequents.append(k[0][1])

testX = []
for a in antecedents:
  prev = [char_to_int[value] for value in a+" "]
  temp = [1]*10
  temp[-len(prev):] = prev
  testX.append(temp)

outputs = []
for test in testX:
	print ("\"", ''.join([int_to_char[value] for value in test]), "\"")
	# generate characters
	for i in range(6):
		x = numpy.reshape(test, (1, len(test), 1))
		x = x / float(n_vocab)
		prediction = model.predict(x, verbose=0)
		index = numpy.argmax(prediction)
		result = int_to_char[index]
		seq_in = [int_to_char[value] for value in test]
		#print(result)
		sys.stdout.write(result)
		test.append(index)
		test = test[1:len(test)]
	outputs.append(''.join([int_to_char[value] for value in test]))

outputs[1:10]

"""This is the final accuracy of the character level LSTM. (Please refer the report for the detailed methodology)"""

count = 0
i = 0
while i < len(outputs):
  if consequents[i] in outputs[i]:
    count += 1
  i += 1

print (count)

