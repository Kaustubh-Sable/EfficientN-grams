# -*- coding: utf-8 -*-
"""Copy of 15 characters window 50 epochs of LSTM Next word Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12P433n16hbcusJ2X4Lf6hGJpZVMswEgE

**LSTM Based N-gram predictor**
"""

import sys
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils

from google.colab import drive
drive.mount('/gdrive')

import os

print(os.listdir('../gdrive/My Drive/NLP'))
os.chdir('../gdrive/My Drive/NLP/')

!pip install tensorflow-gpu==1.14

# load ascii text and covert to lowercase
filename = "LSTM/republic.txt"
raw_text = open(filename, 'r', encoding='utf-8').read()
raw_text = raw_text.lower()

# create mapping of unique chars to integers, and a reverse mapping
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

# summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)
print ("Total Characters: ", n_chars)
print ("Total Vocab: ", n_vocab)

# prepare the dataset of input to output pairs encoded as integers
seq_length = 50
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
	seq_in = raw_text[i:i + seq_length]
	seq_out = raw_text[i + seq_length]
	dataX.append([char_to_int[char] for char in seq_in])
	dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)

n_patterns = len(dataX)
print ("Total Patterns: ", n_patterns)

# reshape X to be [samples, time steps, features]
X = numpy.reshape(dataX, (n_patterns, seq_length, 1))
# normalize
X = X / float(n_vocab)

# one hot encode the output variable
y = np_utils.to_categorical(dataY)
# define the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# define the checkpoint
filepath="weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
history = model.fit(X, y, validation_split=0.2, epochs=50, batch_size=10000, callbacks=callbacks_list)

import gc
gc.collect()

# serialize model to JSON
from keras.models import Sequential
from keras.layers import Dense
from keras.models import model_from_json
import numpy
import os

model_json = model.to_json()
with open("model_10000.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model_1000.h5")
print("Saved model to disk")

import matplotlib.pyplot as plt

#history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)

# Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# load the network weights
"""filename = "model_50.h5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')"""

model

# We will remove double quotes, !, ? $, #, etc
# specify to translate chars 
str1 = ""
# specify to replace with 
str2 = ""
# delete chars 
str3 = "\"!?#$%^&*+"

trg = raw_text
table = trg.maketrans(str1, str2, str3)
raw_text = trg.translate(table)

print(len(raw_text))


words = raw_text.split()
print (len(words))

words = [word.lower() for word in words]

import nltk
freq_dist = nltk.FreqDist(words)


from nltk.util import ngrams

esBigrams = ngrams(words, 2)


import collections
esBigramFreq = collections.Counter(esBigrams)


esTrigrams = ngrams(words, 3)
esTrigramFreq = collections.Counter(esTrigrams)

esTrigramFreq.most_common(10)



esBigramFreq.most_common(10)

esTrigramFreq.most_common(10)

prev = [char_to_int[value] for value in "and "]
temp = [1]*15
temp[-len(prev):] = prev
patternX = temp

print ("Seed:")
print ("\"", ''.join([int_to_char[value] for value in patternX]), "\"")
# generate characters
for i in range(6):
	x = numpy.reshape(patternX, (1, len(patternX), 1))
	x = x / float(n_vocab)
	prediction = model.predict(x, verbose=0)
	index = numpy.argmax(prediction)
	result = int_to_char[index]
	seq_in = [int_to_char[value] for value in patternX]
	#print(result)
	sys.stdout.write(result)
	patternX.append(index)
	patternX = patternX[1:len(patternX)]
print ("\nDone.")

print ("\"", ''.join([int_to_char[value] for value in patternX]), "\"")

