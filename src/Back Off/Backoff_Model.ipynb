{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backoff Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0ho0-yLUN-",
        "colab_type": "text"
      },
      "source": [
        "**Backoff-model:**\n",
        "\n",
        "Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It accomplishes this estimation by backing off through progressively shorter history models under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results.\n",
        "\n",
        "Pros:\n",
        "- Reduction in model size.\n",
        "- Better accuracy as compared to our baseline model.\n",
        "\n",
        "Cons:\n",
        "- Sensitive to noise\n",
        "- It can’t describe the relation between words longer\n",
        "than N.\n",
        "- Success probability degrades with increasing number of nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVIbJig9TnB5",
        "colab_type": "code",
        "outputId": "3038fcd8-ac35-4876-9c0a-aa142302465a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2vPCUXlTRfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "print(os.listdir('../gdrive/My Drive/NLP/Project/Final Project/LSTM'))\n",
        "os.chdir('../gdrive/My Drive/NLP/Project/Final Project/LSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqFg9jQvILPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python library imports:\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize, WhitespaceTokenizer, TweetTokenizer\n",
        "np.random.seed(seed=234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3hu37jZH2Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The below reads in N lines of text from the 40-million-word news corpus I used (provided by SwiftKey for educational purposes).\n",
        "N = 15000\n",
        "with open(\"republic.txt\") as myfile:\n",
        "    articles = [next(myfile) for x in range(N)]\n",
        "joined_articles = [\" \".join(articles)]\n",
        "# The below takes out anything that's not a letter, replacing it with a space, as well as any single letter that is not the pronoun \"I\" or the article \"a.\"\n",
        "def clean_article(article):\n",
        "    art1 = re.sub(\"[^A-Za-z]\", ' ', article)\n",
        "    art2 = re.sub(\"\\s[B-HJ-Zb-hj-z]\\s\", ' ', art1)\n",
        "    art3 = re.sub(\"^[B-HJ-Zb-hj-z]\\s\", ' ', art2)\n",
        "    art4 = re.sub(\"\\s[B-HJ-Zb-hj-z]$\", ' ', art3)\n",
        "    return art4.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QChEJZruH2R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The below breaks up the words into n-grams of length 1 to 5 and puts their counts into a Pandas dataframe with the n-grams as column names.  The maximum number of n-grams can be specified if a large corpus is being used.\n",
        "ngram_bow = CountVectorizer(stop_words = None, preprocessor = clean_article, tokenizer = WhitespaceTokenizer().tokenize, ngram_range=(1,5), max_features = None, max_df = 1.0, min_df = 1, binary = False)\n",
        "ngram_count_sparse = ngram_bow.fit_transform(joined_articles)\n",
        "ngram_count = pd.DataFrame(ngram_count_sparse.toarray())\n",
        "ngram_count.columns = ngram_bow.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQKX1J9TH2co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The below turns the n-gram-count dataframe into a Pandas series with the n-grams as indices for ease of working with the counts.  The second line can be used to limit the n-grams used to those with a count over a cutoff value.\n",
        "sums = ngram_count.sum(axis = 0)\n",
        "sums = sums[sums > 0]\n",
        "ngrams1 = list(sums.index.values)\n",
        "# The function below gives the total number of occurrences of 1-grams in order to calculate 1-gram frequencies\n",
        "def number_of_onegrams(sums):\n",
        "    onegrams = 0\n",
        "    for ng in ngrams1:\n",
        "        ng_split = ng.split(\" \")\n",
        "        if len(ng_split) == 1:\n",
        "            onegrams += sums[ng]\n",
        "    return onegrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmP-5WHDH_t-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The function below makes a series of 1-gram frequencies.  This is the last resort of the back-off algorithm if the n-gram completion does not occur in the corpus with any of the prefix words.\n",
        "def base_freq(og):\n",
        "    freqs = pd.Series()\n",
        "    for ng in ngrams1:\n",
        "        ng_split = ng.split(\" \")\n",
        "        if len(ng_split) == 1:\n",
        "            freqs[ng] = sums[ng] / og\n",
        "    return freqs\n",
        "# For use in later functions so as not to re-calculate multiple times:\n",
        "bf = base_freq(number_of_onegrams(sums))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRIubwocH_1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The function below finds any n-grams that are completions of a given prefix phrase with a specified number (could be zero) of words 'chopped' off the beginning.  For each, it calculates the count ratio of the completion to the (chopped) prefix, tabulating them in a series to be returned by the function.  If the number of chops equals the number of words in the prefix (i.e. all prefix words are chopped), the 1-gram base frequencies are returned.\n",
        "def find_completion_scores(prefix, chops, factor = 0.4):\n",
        "    cs = pd.Series()\n",
        "    prefix_split = prefix.split(\" \")\n",
        "    l = len(prefix_split)\n",
        "    prefix_split_chopped = prefix_split[chops:l]\n",
        "    new_l = l - chops\n",
        "    if new_l == 0:\n",
        "        return factor**chops * bf\n",
        "    prefix_chopped = ' '.join(prefix_split_chopped)\n",
        "    for ng in ngrams1:\n",
        "        ng_split = ng.split(\" \")\n",
        "        if (len(ng_split) == new_l + 1) and (ng_split[0:new_l] == prefix_split_chopped):\n",
        "            cs[ng_split[-1]] = factor**chops * sums[ng] / sums[prefix_chopped]\n",
        "    return cs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yK6PYRtIkXu",
        "colab_type": "code",
        "outputId": "4a1ff308-8469-43e9-c225-7992c8e9edbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Example of completion scores:\n",
        "find_completion_scores('the rest of', 0, 0.4)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "greece     0.043478\n",
              "mankind    0.086957\n",
              "our        0.086957\n",
              "the        0.739130\n",
              "their      0.043478\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEC77d-9JLI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The below tries different numbers of 'chops' up to the length of the prefix to come up with a (still unordered) combined list of scores for potential completions of the prefix.\n",
        "def score_given(given, fact = 0.4):\n",
        "    sg = pd.Series()\n",
        "    given_split = given.split(\" \")\n",
        "    given_length = len(given_split)\n",
        "    for i in range(given_length+1):\n",
        "        fcs = find_completion_scores(given, i, fact)\n",
        "        for i in fcs.index:\n",
        "            if i not in sg.index:\n",
        "                sg[i] = fcs[i]\n",
        "    return sg\n",
        "#The below takes the potential completion scores, puts them in descending order and re-normalizes them as a percentage (pseudo-probability).\n",
        "def score_output(given, fact = 0.4):\n",
        "    sg = score_given(given, fact)\n",
        "    ss = sg.sum()\n",
        "    sg = 100 * sg / ss\n",
        "    sg.sort_values(axis=0, ascending=False, inplace=True)\n",
        "    return round(sg,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7821erQwK57J",
        "colab_type": "code",
        "outputId": "e5b92d36-a4f3-43ec-b4b8-a81d75f2d727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# The below shows that even with a corpus that is way too small, results start to become intuitive.\n",
        "score_output('the rest of', fact = 0.4)[0:15]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the          64.5\n",
              "mankind       7.6\n",
              "our           7.6\n",
              "greece        3.8\n",
              "their         3.8\n",
              "a             0.6\n",
              "them          0.3\n",
              "his           0.3\n",
              "of            0.3\n",
              "all           0.2\n",
              "good          0.2\n",
              "plato         0.2\n",
              "which         0.2\n",
              "justice       0.1\n",
              "knowledge     0.1\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROCpuf5AeaqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYHTFLNJbt-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e9b0d80e-5c64-41a7-c2d4-ff334c855c20"
      },
      "source": [
        "# code to confirm the n-grams generated by the our model by comparing it with actual frequency based output\n",
        "blogs = open('republic.txt', 'rt')\n",
        "text = blogs.read()\n",
        "blogs.close()\n",
        "corpus = text\n",
        "# We will remove double quotes, !, ? $, #, etc\n",
        "# specify to translate chars \n",
        "str1 = \"\"\n",
        "# specify to replace with \n",
        "str2 = \"\"\n",
        "# delete chars \n",
        "str3 = \"\\\"!?#$%^&*+\"\n",
        "\n",
        "trg = corpus\n",
        "table = trg.maketrans(str1, str2, str3)\n",
        "corpus = trg.translate(table)\n",
        "words = corpus.split()\n",
        "\n",
        "esQuadgrams = ngrams(words, 4)\n",
        "esQuadgramFreq = collections.Counter(esQuadgrams)\n",
        "esQuadgramFreq.most_common(10)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('for', 'the', 'sake', 'of'), 28),\n",
              " (('Very', 'true,', 'he', 'said.'), 26),\n",
              " (('the', 'interest', 'of', 'the'), 25),\n",
              " (('What', 'do', 'you', 'mean'), 24),\n",
              " (('at', 'the', 'same', 'time'), 22),\n",
              " (('the', 'rest', 'of', 'the'), 21),\n",
              " (('Yes,', 'he', 'said,', 'that'), 21),\n",
              " (('in', 'the', 'case', 'of'), 20),\n",
              " (('as', 'well', 'as', 'of'), 18),\n",
              " (('the', 'idea', 'of', 'good'), 18)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ1U2LlZa6FK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e854e424-702b-48ba-cb04-368b78f93bc5"
      },
      "source": [
        "# code to test the LSTM generated output with the frequency-based output over 100 predictions.\n",
        "testSet = esQuadgramFreq.most_common(100)\n",
        "count=0\n",
        "for quadgramTup in testSet:\n",
        "  quadgram=quadgramTup[0]\n",
        "  seed_text=quadgram[0]+\" \"+quadgram[1]+\" \"+quadgram[2]\n",
        "  expected_text=quadgram[3]\n",
        "  generated = score_output(seed_text, fact = 0.4)[0:1].index[0]\n",
        "  if generated.strip()==expected_text:\n",
        "    count+=1\n",
        "print (\"Accuracy: \",count/100)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFkT2aibeo9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}